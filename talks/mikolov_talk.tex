\documentclass{article}

\title{Limits of Deep Learning: Challenges in learning and generalization}
\author{Hugo Cisneros}
\date{April 30th 2019}

\begin{document}
\maketitle

Difference between generalization and memorization:
Generalizing means infering novel conclusions from previous experiences. Exemple
of a calculator that learns all addtions below a certain number but cannot
extrapolate.

The case of DNNs. Generalization is hard to understand and or grasp.
They for sure have limits and can be clearly seen.

The question of Language modelling: it is AI complete to be able to assing a
probability to all possible sentences. These models could either build goosd
quality chatbots and generate novel texts.

The tendence has overall been: the larger the model the better. But is this a
way to approach the solution of the AI-complete problem ? The situations where
these models fail show the shortcomings of these methods.

In translation: LM can be used to predict the next sentence from the previous.
But for long sentences, it is not even able to predict the same sentence twice.
The issue is that SGD won't ever generalize from the training exemples, which is
fine in a practical setting but not to achieve the longer-term goal mentioned
above.

For example: the stack augmented RNNs use a stack to memorize some of the data,
the stack has PUSH , POP and NO-OP. With two or more stacks, the computational
model is Turing complete. Joulin and Mikolov NIPS 2015 \textit{Inferring
  algorithmic patterns with stack augmented recurrent nets}
Two training procedures:
\begin{itemize}
\item Search based (genetic algorithm)
  \item gradient based (SGD)
\end{itemize}
It works well for memorization (better than RNN and LSTMS) and also for binary
addition tasks with good generalization properties.

The idea of algorithmic transfer learning:
high-level transfer learning tasks are not very well done by classical models.
With RNNs, trying to solve a slightly more complex task usually fails. Some
ideas: we need less supervision, possibly no SGD and/or convergence (it keeps
learning, which is understandable). A more fundamental/basic model than RNNs?
memory, learning, tasks, rewards, etc. might be different properties of a more
complete system.


\end{document}
